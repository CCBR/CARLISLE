{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CARLISLE \u00b6 C ut A nd R un ana L ys IS pipe L in E This snakemake pipeline is built to run on Biowulf . For comments/suggestions/advice please contact CCBR_Pipeliner@mail.nih.gov . For detailed documentation on running the pipeline, please view the website: https://CCBR.github.io/CARLISLE/ Workflow \u00b6 The CARLISLE pipeline was developed in support of NIH Dr Vassiliki Saloura's Laboratory and Dr Javed Khan's Laboratory . It has been developed and tested solely on NIH HPC Biowulf .","title":"Home"},{"location":"#carlisle","text":"C ut A nd R un ana L ys IS pipe L in E This snakemake pipeline is built to run on Biowulf . For comments/suggestions/advice please contact CCBR_Pipeliner@mail.nih.gov . For detailed documentation on running the pipeline, please view the website: https://CCBR.github.io/CARLISLE/","title":"CARLISLE"},{"location":"#workflow","text":"The CARLISLE pipeline was developed in support of NIH Dr Vassiliki Saloura's Laboratory and Dr Javed Khan's Laboratory . It has been developed and tested solely on NIH HPC Biowulf .","title":"Workflow"},{"location":"changelog/","text":"CARLISLE development version \u00b6 CARLISLE 2.7.2 \u00b6 Fix how singularity bind paths are set. (#187, @kelly-sovacool) Fix the DESeq rule when using the hs1 T2T genome. (#187, @kelly-sovacool, @wong-nw) Fix incorrect jobby version (needs v0.4). (#194, @kelly-sovacool) Fix jobby usage to output stdout/stderr to log file. (#196, @kopardev) Fix numpy error. (#198, @kelly-sovacool) Fix logic error with scaling factor calcutions. (#195, @kopardev) Run MACS2 with control by default. (#195, @kopardev) Make sure submit_slurm.sbatch is not overwritten. (#195, @kopardev) Handle case when sample names are subsets of each other. (#195, @kopardev) CARLISLE 2.7.1 \u00b6 Remove deprecated 'ccr' partition. (#176, @kopardev) CARLISLE 2.7.0 \u00b6 Now depends on ccbr_tools v0.4 for updated jobby & spooker utilities. (#168, @kelly-sovacool) CARLISLE 2.6.4 \u00b6 Fix driver script: do not try to load the shared conda environment. (#166, @kelly-sovacool) Now depends on ccbr_tools v0.4 for updated jobby & spooker utilities. (#168, @kelly-sovacool) CARLISLE 2.6.3 \u00b6 Minor documentation update to use readthedocs theme. (#162, @kelly-sovacool) Fix deprecation warning during initialization. (#163, @kelly-sovacool) CARLISLE 2.6.2 \u00b6 Documentation improvements. (#154, @kelly-sovacool) If jobby & spooker are not available, try adding them to the path on workflow completion. (#155, @kelly-sovacool) Fix bug that flipped library normalization scaling factor (#157, @epehrsson) CARLISLE 2.6.1 \u00b6 Load the module for snakemake v7, but do not specify the minor and patch versions. (#149, @kelly-sovacool) CARLISLE 2.6.0 \u00b6 Bug fixes \u00b6 Bug fixes for DESeq (#127, @epehrsson) Removes single-sample group check for DESeq. Increases memory for DESeq. Ensures control replicate number is an integer. Fixes FDR cutoff misassigned to log2FC cutoff. Fixes no_dedup variable names in library normalization scripts. Fig bug that added nonexistent directories to the singularity bind paths. (#135, @kelly-sovacool) Containerize rules that require R ( deseq , go_enrichment , and spikein_assessment ) to fix installation issues with common R library path. (#129, @kelly-sovacool) The Rlib_dir and Rpkg_config config options have been removed as they are no longer needed. New features \u00b6 New visualizations: (#132, @epehrsson) New rules cov_correlation , homer_enrich , combine_homer , count_peaks Add peak caller to MACS2 peak xls filename New parameters in the config file to make certain rules optional: (#133, @kelly-sovacool) GO enrichment is controlled by run_go_enrichment (default: false ) ROSE is controlled by run_rose (default: false ) New --singcache argument to provide a singularity cache dir location. The singularity cache dir is automatically set inside /data/$USER/ or $WORKDIR/ if --singcache is not provided. (#143, @kelly-sovacool) Misc \u00b6 The singularity version is no longer specified, per request of the biowulf admins. (#139, @kelly-sovacool) Minor documentation updates. (#146, @kelly-sovacool) CARLISLE v2.5.0 \u00b6 Refactors R packages to a common source location (#118, @slsevilla) Adds a --force flag to allow for re-initialization of a workdir (#97, @slsevilla) Fixes error with testrun in DESEQ2 (#113, @slsevilla) Decreases the number of samples being run with testrun, essentially running tinytest as default and removing tinytest as an option (#115, @slsevilla) Reads version from VERSION file instead of github repo link (#96, #112, @slsevilla) Added a CHANGELOG (#116, @slsevilla) Fix: RNA report bug, caused by hard-coding of PC1-3, when only PC1-2 were generated (#104, @slsevilla) Minor documentation improvements. (#100, @kelly-sovacool) Fix: allow printing the version or help message even if singularity is not in the path. (#110, @kelly-sovacool) CARLISLE v2.4.1 \u00b6 Add GitHub Action to add issues/PRs to personal project boards by @kelly-sovacool in #95 Create install script by @kelly-sovacool in #93 feat: use summits bed for homer input; save temporary files; fix deseq2 bug by @slsevilla in #108 docs: adding citation and DOI to pipeline by @slsevilla in #107 Test a dryrun with GitHub Actions by @kelly-sovacool in #94 CARLISLE v2.4.0 \u00b6 Feature- Merged Yue's fork, adding DEEPTOOLS by @slsevilla in #85 Feature- Added tracking features from SPOOK by @slsevilla in #88 Feature - Dev test run completed by @slsevilla in #89 Bug - Fixed bugs related to Biowulf transition CARLISLE v2.1.0 \u00b6 enhancement update gopeaks resources change SEACR to run \"norm\" without spikein controls, \"non\" with spikein controls update docs for changes; provide extra troubleshooting guidance fix GoEnrich bug for failed plots CARLISLE v2.0.1 \u00b6 fix error when contrasts set to \"N\" adjust goenrich resources to be more efficient CARLISLE 2.0.0 \u00b6 Add a MAPQ filter to samtools (rule align) Add GoPeaks MultiQC module Allow for library normalization to occur during first pass Add --broad-cutoff to MACS2 broad peak calling for MACS2 Create a spike in QC report Reorganize file structure to help with qthreshold folder Update variable names of all peak caller Merge rules with input/output/wildcard congruency Convert the \"spiked\" variable to \"norm_method Add name of control used to MACS2 peaks Running extra control:sample comparisons that are not needed improved resource allocation test data originally included 1475 jobs, this version includes 1087 jobs (reduction of 25%) despite including additional features moved ~12% of all jobs to local deployment (within SLURM submission) CARLISLE 1.2.0 \u00b6 merge increases to resources; update workflow img, contributions CARLISLE 1.1.1 \u00b6 patch for gz bigbed bug CARLISLE 1.1.0 \u00b6 add broad-cutoff to macs2 broad peaks param settings add non.stringent and non.relaxed to annotation options merge DESEQ and DESEQ2 rules together identify some files as temp CARLISLE 1.0.1 \u00b6 contains patch for DESEQ error with non hs1 reference samples","title":"Changelog"},{"location":"changelog/#carlisle-development-version","text":"","title":"CARLISLE development version"},{"location":"changelog/#carlisle-272","text":"Fix how singularity bind paths are set. (#187, @kelly-sovacool) Fix the DESeq rule when using the hs1 T2T genome. (#187, @kelly-sovacool, @wong-nw) Fix incorrect jobby version (needs v0.4). (#194, @kelly-sovacool) Fix jobby usage to output stdout/stderr to log file. (#196, @kopardev) Fix numpy error. (#198, @kelly-sovacool) Fix logic error with scaling factor calcutions. (#195, @kopardev) Run MACS2 with control by default. (#195, @kopardev) Make sure submit_slurm.sbatch is not overwritten. (#195, @kopardev) Handle case when sample names are subsets of each other. (#195, @kopardev)","title":"CARLISLE 2.7.2"},{"location":"changelog/#carlisle-271","text":"Remove deprecated 'ccr' partition. (#176, @kopardev)","title":"CARLISLE 2.7.1"},{"location":"changelog/#carlisle-270","text":"Now depends on ccbr_tools v0.4 for updated jobby & spooker utilities. (#168, @kelly-sovacool)","title":"CARLISLE 2.7.0"},{"location":"changelog/#carlisle-264","text":"Fix driver script: do not try to load the shared conda environment. (#166, @kelly-sovacool) Now depends on ccbr_tools v0.4 for updated jobby & spooker utilities. (#168, @kelly-sovacool)","title":"CARLISLE 2.6.4"},{"location":"changelog/#carlisle-263","text":"Minor documentation update to use readthedocs theme. (#162, @kelly-sovacool) Fix deprecation warning during initialization. (#163, @kelly-sovacool)","title":"CARLISLE 2.6.3"},{"location":"changelog/#carlisle-262","text":"Documentation improvements. (#154, @kelly-sovacool) If jobby & spooker are not available, try adding them to the path on workflow completion. (#155, @kelly-sovacool) Fix bug that flipped library normalization scaling factor (#157, @epehrsson)","title":"CARLISLE 2.6.2"},{"location":"changelog/#carlisle-261","text":"Load the module for snakemake v7, but do not specify the minor and patch versions. (#149, @kelly-sovacool)","title":"CARLISLE 2.6.1"},{"location":"changelog/#carlisle-260","text":"","title":"CARLISLE 2.6.0"},{"location":"changelog/#bug-fixes","text":"Bug fixes for DESeq (#127, @epehrsson) Removes single-sample group check for DESeq. Increases memory for DESeq. Ensures control replicate number is an integer. Fixes FDR cutoff misassigned to log2FC cutoff. Fixes no_dedup variable names in library normalization scripts. Fig bug that added nonexistent directories to the singularity bind paths. (#135, @kelly-sovacool) Containerize rules that require R ( deseq , go_enrichment , and spikein_assessment ) to fix installation issues with common R library path. (#129, @kelly-sovacool) The Rlib_dir and Rpkg_config config options have been removed as they are no longer needed.","title":"Bug fixes"},{"location":"changelog/#new-features","text":"New visualizations: (#132, @epehrsson) New rules cov_correlation , homer_enrich , combine_homer , count_peaks Add peak caller to MACS2 peak xls filename New parameters in the config file to make certain rules optional: (#133, @kelly-sovacool) GO enrichment is controlled by run_go_enrichment (default: false ) ROSE is controlled by run_rose (default: false ) New --singcache argument to provide a singularity cache dir location. The singularity cache dir is automatically set inside /data/$USER/ or $WORKDIR/ if --singcache is not provided. (#143, @kelly-sovacool)","title":"New features"},{"location":"changelog/#misc","text":"The singularity version is no longer specified, per request of the biowulf admins. (#139, @kelly-sovacool) Minor documentation updates. (#146, @kelly-sovacool)","title":"Misc"},{"location":"changelog/#carlisle-v250","text":"Refactors R packages to a common source location (#118, @slsevilla) Adds a --force flag to allow for re-initialization of a workdir (#97, @slsevilla) Fixes error with testrun in DESEQ2 (#113, @slsevilla) Decreases the number of samples being run with testrun, essentially running tinytest as default and removing tinytest as an option (#115, @slsevilla) Reads version from VERSION file instead of github repo link (#96, #112, @slsevilla) Added a CHANGELOG (#116, @slsevilla) Fix: RNA report bug, caused by hard-coding of PC1-3, when only PC1-2 were generated (#104, @slsevilla) Minor documentation improvements. (#100, @kelly-sovacool) Fix: allow printing the version or help message even if singularity is not in the path. (#110, @kelly-sovacool)","title":"CARLISLE v2.5.0"},{"location":"changelog/#carlisle-v241","text":"Add GitHub Action to add issues/PRs to personal project boards by @kelly-sovacool in #95 Create install script by @kelly-sovacool in #93 feat: use summits bed for homer input; save temporary files; fix deseq2 bug by @slsevilla in #108 docs: adding citation and DOI to pipeline by @slsevilla in #107 Test a dryrun with GitHub Actions by @kelly-sovacool in #94","title":"CARLISLE v2.4.1"},{"location":"changelog/#carlisle-v240","text":"Feature- Merged Yue's fork, adding DEEPTOOLS by @slsevilla in #85 Feature- Added tracking features from SPOOK by @slsevilla in #88 Feature - Dev test run completed by @slsevilla in #89 Bug - Fixed bugs related to Biowulf transition","title":"CARLISLE v2.4.0"},{"location":"changelog/#carlisle-v210","text":"enhancement update gopeaks resources change SEACR to run \"norm\" without spikein controls, \"non\" with spikein controls update docs for changes; provide extra troubleshooting guidance fix GoEnrich bug for failed plots","title":"CARLISLE v2.1.0"},{"location":"changelog/#carlisle-v201","text":"fix error when contrasts set to \"N\" adjust goenrich resources to be more efficient","title":"CARLISLE v2.0.1"},{"location":"changelog/#carlisle-200","text":"Add a MAPQ filter to samtools (rule align) Add GoPeaks MultiQC module Allow for library normalization to occur during first pass Add --broad-cutoff to MACS2 broad peak calling for MACS2 Create a spike in QC report Reorganize file structure to help with qthreshold folder Update variable names of all peak caller Merge rules with input/output/wildcard congruency Convert the \"spiked\" variable to \"norm_method Add name of control used to MACS2 peaks Running extra control:sample comparisons that are not needed improved resource allocation test data originally included 1475 jobs, this version includes 1087 jobs (reduction of 25%) despite including additional features moved ~12% of all jobs to local deployment (within SLURM submission)","title":"CARLISLE 2.0.0"},{"location":"changelog/#carlisle-120","text":"merge increases to resources; update workflow img, contributions","title":"CARLISLE 1.2.0"},{"location":"changelog/#carlisle-111","text":"patch for gz bigbed bug","title":"CARLISLE 1.1.1"},{"location":"changelog/#carlisle-110","text":"add broad-cutoff to macs2 broad peaks param settings add non.stringent and non.relaxed to annotation options merge DESEQ and DESEQ2 rules together identify some files as temp","title":"CARLISLE 1.1.0"},{"location":"changelog/#carlisle-101","text":"contains patch for DESEQ error with non hs1 reference samples","title":"CARLISLE 1.0.1"},{"location":"contributing/","text":"Contributing to CARLISLE \u00b6 Proposing changes with issues \u00b6 If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed. If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it. Pull request process \u00b6 We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to CARLISLE. Clone the repo \u00b6 If you are a member of CCBR , you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once. git clone https://github.com/CCBR/CARLISLE Cloning into 'CARLISLE'... remote: Enumerating objects: 1136, done. remote: Counting objects: 100% (463/463), done. remote: Compressing objects: 100% (357/357), done. remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673 Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done. Resolving deltas: 100% (530/530), done. cd CARLISLE If this is your first time cloning the repo, you may need to install dependencies \u00b6 Install snakemake and singularity or docker if needed (biowulf already has these available as modules). Install the python dependencies with pip pip install . If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" conda activate py311 Install pre-commit if you don't already have it. Then from the repo's root directory, run pre-commit install This will install the repo's pre-commit hooks. You'll only need to do this step the first time you clone the repo. Create a branch \u00b6 Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue. # create a new branch and switch to it git branch iss-10 git switch iss-10 Switched to a new branch 'iss-10' Make your changes \u00b6 Edit the code, write and run tests, and update the documentation as needed. test \u00b6 Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest . If you change the workflow , please run the workflow with the test profile and make sure your new feature or bug fix works as intended. document \u00b6 If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/ . Commit and push your changes \u00b6 If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/ First, add the files that you changed to the staging area: git add path/to/changed/files/ Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat , fix , docs , etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages. git commit -m 'feat: create function for awesome feature' pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Failed hook id: trailing-whitespace exit code: 1 files were modified by this hook > Fixing path/to/changed/files/file.txt > codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command: git add path/to/changed/files/file.txt git commit -m 'feat: create function for awesome feature' This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Passed codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped Conventional Commit......................................................Passed > [iss-10 9ff256e] feat: create function for awesome feature 1 file changed, 22 insertions(+), 3 deletions(-) Finally, push your changes to GitHub: git push If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch: git push --set-upstream origin iss-10 Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 10 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'iss-10' on GitHub by visiting: remote: https://github.com/CCBR/CARLISLE/pull/new/iss-10 remote: To https://github.com/CCBR/CARLISLE > > [new branch] iss-10 -> iss-10 branch 'iss-10' set up to track 'origin/iss-10'. We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/CARLISLE/tree/<your-branch-name> (replace <your-branch-name> with the actual name of your branch). Create the PR \u00b6 Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/CARLISLE/pull/new/ Select the branch you just pushed: Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <!-- and --> ) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it. Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready. Wait for a maintainer to review your PR \u00b6 We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/ . The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR. Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution! After your PR has been merged \u00b6 After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes: git checkout main git pull It's a good idea to run git pull before creating a new branch so it will start from the most recent commits in main. Helpful links for more information \u00b6 GitHub Flow semantic versioning guidelines changelog guidelines tidyverse code review principles reproducible examples nf-core extensions for VS Code","title":"Contributing to CARLISLE"},{"location":"contributing/#contributing-to-carlisle","text":"","title":"Contributing to CARLISLE"},{"location":"contributing/#proposing-changes-with-issues","text":"If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed. If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it.","title":"Proposing changes with issues"},{"location":"contributing/#pull-request-process","text":"We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to CARLISLE.","title":"Pull request process"},{"location":"contributing/#clone-the-repo","text":"If you are a member of CCBR , you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once. git clone https://github.com/CCBR/CARLISLE Cloning into 'CARLISLE'... remote: Enumerating objects: 1136, done. remote: Counting objects: 100% (463/463), done. remote: Compressing objects: 100% (357/357), done. remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673 Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done. Resolving deltas: 100% (530/530), done. cd CARLISLE","title":"Clone the repo"},{"location":"contributing/#if-this-is-your-first-time-cloning-the-repo-you-may-need-to-install-dependencies","text":"Install snakemake and singularity or docker if needed (biowulf already has these available as modules). Install the python dependencies with pip pip install . If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" conda activate py311 Install pre-commit if you don't already have it. Then from the repo's root directory, run pre-commit install This will install the repo's pre-commit hooks. You'll only need to do this step the first time you clone the repo.","title":"If this is your first time cloning the repo, you may need to install dependencies"},{"location":"contributing/#create-a-branch","text":"Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue. # create a new branch and switch to it git branch iss-10 git switch iss-10 Switched to a new branch 'iss-10'","title":"Create a branch"},{"location":"contributing/#make-your-changes","text":"Edit the code, write and run tests, and update the documentation as needed.","title":"Make your changes"},{"location":"contributing/#test","text":"Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest . If you change the workflow , please run the workflow with the test profile and make sure your new feature or bug fix works as intended.","title":"test"},{"location":"contributing/#document","text":"If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/ .","title":"document"},{"location":"contributing/#commit-and-push-your-changes","text":"If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/ First, add the files that you changed to the staging area: git add path/to/changed/files/ Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat , fix , docs , etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages. git commit -m 'feat: create function for awesome feature' pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Failed hook id: trailing-whitespace exit code: 1 files were modified by this hook > Fixing path/to/changed/files/file.txt > codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command: git add path/to/changed/files/file.txt git commit -m 'feat: create function for awesome feature' This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Passed codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped Conventional Commit......................................................Passed > [iss-10 9ff256e] feat: create function for awesome feature 1 file changed, 22 insertions(+), 3 deletions(-) Finally, push your changes to GitHub: git push If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch: git push --set-upstream origin iss-10 Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 10 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'iss-10' on GitHub by visiting: remote: https://github.com/CCBR/CARLISLE/pull/new/iss-10 remote: To https://github.com/CCBR/CARLISLE > > [new branch] iss-10 -> iss-10 branch 'iss-10' set up to track 'origin/iss-10'. We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/CARLISLE/tree/<your-branch-name> (replace <your-branch-name> with the actual name of your branch).","title":"Commit and push your changes"},{"location":"contributing/#create-the-pr","text":"Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/CARLISLE/pull/new/ Select the branch you just pushed: Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <!-- and --> ) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it. Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready.","title":"Create the PR"},{"location":"contributing/#wait-for-a-maintainer-to-review-your-pr","text":"We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/ . The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR. Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!","title":"Wait for a maintainer to review your PR"},{"location":"contributing/#after-your-pr-has-been-merged","text":"After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes: git checkout main git pull It's a good idea to run git pull before creating a new branch so it will start from the most recent commits in main.","title":"After your PR has been merged"},{"location":"contributing/#helpful-links-for-more-information","text":"GitHub Flow semantic versioning guidelines changelog guidelines tidyverse code review principles reproducible examples nf-core extensions for VS Code","title":"Helpful links for more information"},{"location":"user-guide/contributions/","text":"Contributions \u00b6 The following members contributed to the development of the CARLISLE pipeline: Samantha Sevilla Vishal Koparde Kelly Sovacool Hsien-chao Chou Sohyoung Kim Yue Hu Vassiliki Saloura VK, SS, SK, HC contributed to the generating the source code and all members contributed to the main concepts and analysis.","title":"Contributions"},{"location":"user-guide/contributions/#contributions","text":"The following members contributed to the development of the CARLISLE pipeline: Samantha Sevilla Vishal Koparde Kelly Sovacool Hsien-chao Chou Sohyoung Kim Yue Hu Vassiliki Saloura VK, SS, SK, HC contributed to the generating the source code and all members contributed to the main concepts and analysis.","title":"Contributions"},{"location":"user-guide/getting-started/","text":"Getting Started \u00b6 Introduction \u00b6 The CARLISLE Pipeline is designed for the analysis of CUT&RUN and CUT&Tag datasets, two closely related chromatin profiling methods that provide high-resolution maps of protein-DNA interactions. Both techniques use antibody-targeted micrococcal nuclease (MNase) or Tn5 transposase to selectively cleave or tag DNA fragments bound by proteins of interest. The resulting DNA is then purified and sequenced to identify binding sites across the genome. CUT&RUN, in particular, offers improved signal-to-noise ratio and lower background compared to traditional ChIP-seq , making it highly efficient even with limited input material. While CUT&Tag uses an engineered transposase to insert sequencing adapters directly into chromatin fragments, CUT&RUN relies on nuclease digestion and separate library preparation. Despite these differences, the downstream computational workflow\u2014including alignment, filtering, deduplication, and peak calling\u2014is largely similar. The CARLISLE pipeline supports both methods, offering flexibility for diverse chromatin profiling experiments and maintaining compatibility with established Biowulf HPC environments. Inspired by the nf-core/cutandrun pipeline, CARLISLE incorporates modular and reproducible analysis steps that emphasize transparency and scalability. It begins with raw FASTQ files, performing adapter trimming followed by alignment using Bowtie2 . Linear deduplication is an important part of the process\u2014especially for CUT&RUN and CUT&Tag data\u2014to remove PCR artifacts and ensure accurate quantification of unique DNA fragments. Normalization is performed using either user-provided spike-in controls (e.g., E. coli ) or library size-based scaling. Peak calling is then executed using MACS2 , SEACR , and GoPeaks \u2014with GoPeaks recommended for its robust performance on CUT&RUN data. Following peak calling, CARLISLE annotates results and summarizes them into detailed reports, with optional differential analysis handled via DESeq2 . Quality control metrics are generated using FastQC and MultiQC , while enrichment and annotation are supported through HOMER , ROSE , and ChIP-Enrich . Setup Dependencies \u00b6 Note: All dependencies are currently module-loaded from the Biowulf HPC environment . However, future versions of the CARLISLE pipeline will transition to a fully containerized setup using Singularity/Apptainer or Docker for improved reproducibility and portability. CARLISLE relies on several dependencies, most of which are pre-installed and auto-loaded on Biowulf via the ccbrpipeliner module. Tool Module / Version bedtools bedtools/2.30.0 bedops bedops/2.4.40 bowtie2 bowtie/2-2.4.2 cutadapt cutadapt/1.18 fastqc fastqc/0.11.9 fastq_screen fastq_screen/0.15.2 fastq_val /data/CCBR_Pipeliner/iCLIP/bin/fastQValidator fastxtoolkit fastxtoolkit/0.0.14 gopeaks git clone https://github.com/maxsonBraunLab/gopeaks macs2 macs/2.2.7.1 multiqc multiqc/1.9 perl perl/5.34.0 picard picard/2.26.9 python python/3.7 R R/4.2.2 rose ROSE/1.3.1 samtools samtools/1.15 seacr seacr/1.4-beta.2 ucsc ucsc/407 Login to the Cluster \u00b6 CARLISLE has been tested and validated on NIH Biowulf HPC . # SSH into the Biowulf head node ssh -Y $USER @biowulf.nih.gov Move to your project directory before proceeding. Load an Interactive Session \u00b6 An interactive session is required before executing any CARLISLE sub-commands, even if the job will later be submitted to the cluster. # Request an interactive node sinteractive --time = 12 :00:00 --mem = 8gb --cpus-per-task = 4 --pty bash Load Dependencies \u00b6 Load the CARLISLE environment by activating the ccbrpipeliner module: module load ccbrpipeliner/8 Check which version is currently active: carlisle --version [ + ] Loading singularity 4 .2.2 on cn0001 [ + ] Loading snakemake 7 .32.4 Pipeline Dir: /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1 Version: 2 .7.1 carlisle --help [+] Loading singularity 4.2.2 on cn0001 [+] Loading snakemake 7.32.4 Pipeline Dir: /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1 /spin1/home/linux/kopardevn/carlisle --> run CARLISLE Cut And Run anaLysIS pipeLinE USAGE: bash /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1/carlisle -m/--runmode=<RUNMODE> -w/--workdir=<WORKDIR> Required Arguments: 1. RUNMODE: [Type: String] Valid options: *) init : initialize workdir *) run : run with slurm *) reset : DELETE workdir dir and re-init it *) dryrun : dry run snakemake to generate DAG *) unlock : unlock workdir if locked by snakemake *) runlocal : run without submitting to sbatch *) runtest: run on cluster with included hg38 test dataset 2. WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions. Optional Arguments: *) -f / --force: Force flag will re-initialize a previously initialized workdir","title":"Getting Started"},{"location":"user-guide/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"user-guide/getting-started/#introduction","text":"The CARLISLE Pipeline is designed for the analysis of CUT&RUN and CUT&Tag datasets, two closely related chromatin profiling methods that provide high-resolution maps of protein-DNA interactions. Both techniques use antibody-targeted micrococcal nuclease (MNase) or Tn5 transposase to selectively cleave or tag DNA fragments bound by proteins of interest. The resulting DNA is then purified and sequenced to identify binding sites across the genome. CUT&RUN, in particular, offers improved signal-to-noise ratio and lower background compared to traditional ChIP-seq , making it highly efficient even with limited input material. While CUT&Tag uses an engineered transposase to insert sequencing adapters directly into chromatin fragments, CUT&RUN relies on nuclease digestion and separate library preparation. Despite these differences, the downstream computational workflow\u2014including alignment, filtering, deduplication, and peak calling\u2014is largely similar. The CARLISLE pipeline supports both methods, offering flexibility for diverse chromatin profiling experiments and maintaining compatibility with established Biowulf HPC environments. Inspired by the nf-core/cutandrun pipeline, CARLISLE incorporates modular and reproducible analysis steps that emphasize transparency and scalability. It begins with raw FASTQ files, performing adapter trimming followed by alignment using Bowtie2 . Linear deduplication is an important part of the process\u2014especially for CUT&RUN and CUT&Tag data\u2014to remove PCR artifacts and ensure accurate quantification of unique DNA fragments. Normalization is performed using either user-provided spike-in controls (e.g., E. coli ) or library size-based scaling. Peak calling is then executed using MACS2 , SEACR , and GoPeaks \u2014with GoPeaks recommended for its robust performance on CUT&RUN data. Following peak calling, CARLISLE annotates results and summarizes them into detailed reports, with optional differential analysis handled via DESeq2 . Quality control metrics are generated using FastQC and MultiQC , while enrichment and annotation are supported through HOMER , ROSE , and ChIP-Enrich .","title":"Introduction"},{"location":"user-guide/getting-started/#setup-dependencies","text":"Note: All dependencies are currently module-loaded from the Biowulf HPC environment . However, future versions of the CARLISLE pipeline will transition to a fully containerized setup using Singularity/Apptainer or Docker for improved reproducibility and portability. CARLISLE relies on several dependencies, most of which are pre-installed and auto-loaded on Biowulf via the ccbrpipeliner module. Tool Module / Version bedtools bedtools/2.30.0 bedops bedops/2.4.40 bowtie2 bowtie/2-2.4.2 cutadapt cutadapt/1.18 fastqc fastqc/0.11.9 fastq_screen fastq_screen/0.15.2 fastq_val /data/CCBR_Pipeliner/iCLIP/bin/fastQValidator fastxtoolkit fastxtoolkit/0.0.14 gopeaks git clone https://github.com/maxsonBraunLab/gopeaks macs2 macs/2.2.7.1 multiqc multiqc/1.9 perl perl/5.34.0 picard picard/2.26.9 python python/3.7 R R/4.2.2 rose ROSE/1.3.1 samtools samtools/1.15 seacr seacr/1.4-beta.2 ucsc ucsc/407","title":"Setup Dependencies"},{"location":"user-guide/getting-started/#login-to-the-cluster","text":"CARLISLE has been tested and validated on NIH Biowulf HPC . # SSH into the Biowulf head node ssh -Y $USER @biowulf.nih.gov Move to your project directory before proceeding.","title":"Login to the Cluster"},{"location":"user-guide/getting-started/#load-an-interactive-session","text":"An interactive session is required before executing any CARLISLE sub-commands, even if the job will later be submitted to the cluster. # Request an interactive node sinteractive --time = 12 :00:00 --mem = 8gb --cpus-per-task = 4 --pty bash","title":"Load an Interactive Session"},{"location":"user-guide/getting-started/#load-dependencies","text":"Load the CARLISLE environment by activating the ccbrpipeliner module: module load ccbrpipeliner/8 Check which version is currently active: carlisle --version [ + ] Loading singularity 4 .2.2 on cn0001 [ + ] Loading snakemake 7 .32.4 Pipeline Dir: /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1 Version: 2 .7.1 carlisle --help [+] Loading singularity 4.2.2 on cn0001 [+] Loading snakemake 7.32.4 Pipeline Dir: /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1 /spin1/home/linux/kopardevn/carlisle --> run CARLISLE Cut And Run anaLysIS pipeLinE USAGE: bash /vf/users/CCBR_Pipeliner/Pipelines/CARLISLE/.v2.7.1/carlisle -m/--runmode=<RUNMODE> -w/--workdir=<WORKDIR> Required Arguments: 1. RUNMODE: [Type: String] Valid options: *) init : initialize workdir *) run : run with slurm *) reset : DELETE workdir dir and re-init it *) dryrun : dry run snakemake to generate DAG *) unlock : unlock workdir if locked by snakemake *) runlocal : run without submitting to sbatch *) runtest: run on cluster with included hg38 test dataset 2. WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions. Optional Arguments: *) -f / --force: Force flag will re-initialize a previously initialized workdir","title":"Load Dependencies"},{"location":"user-guide/output/","text":"Expected Outputs \u00b6 Upon successful completion, CARLISLE generates a comprehensive directory structure under WORKDIR/results . Each subdirectory contains outputs corresponding to specific stages of the workflow \u2014 from raw alignment statistics to annotated peak results and quality control summaries. Directory Overview \u00b6 alignment_stats/ \u2013 Contains detailed alignment reports for each sample, including mapping efficiency, read depth, and spike-in alignment metrics. bam/ \u2013 Stores sorted and indexed BAM files for all samples. This directory also includes per-sample and spike-in alignment statistics useful for downstream normalization and QC. bedgraph/ \u2013 Includes BEDGRAPH coverage tracks summarizing read density across the genome. These files serve as intermediates for visualization and peak-calling validation. bigwig/ \u2013 Contains BigWig files generated from normalized coverage data, suitable for visualization in genome browsers such as UCSC Genome Browser or IGV . fragments/ \u2013 Stores fragment length distributions and deduplicated fragment data (particularly important for CUT&RUN and CUT&Tag experiments). Useful for assessing fragment size enrichment and MNase digestion efficiency. peaks/ \u2013 The core results directory containing called peaks, differential comparisons, and annotations. Subdirectories are organized by quality thresholds (e.g., 0.05 , 0.01 ), representing the significance cutoffs applied during peak calling. Each quality threshold directory includes: contrasts/ \u2013 Contains results of differential binding analyses defined in the contrast manifest. <peak_caller>/ \u2013 Subdirectories for each peak caller (e.g., macs2 , seacr , gopeaks ). Each includes raw peak calls and annotated results. annotation/ \u2013 Contains enriched feature and pathway analyses: go_enrichment/ \u2013 Results from ChIP-Enrich gene set enrichment, generated when run_go_enrichment: true is enabled. homer/ \u2013 Output from HOMER motif discovery and annotation. rose/ \u2013 Output from ROSE super-enhancer analysis, generated when run_rose: true is specified. qc/ \u2013 Centralized quality control directory containing comprehensive MultiQC summaries, FastQC metrics, and spike-in normalization reports (when applicable). Example Directory Layout \u00b6 Below is an example of the CARLISLE output structure for a typical CUT&RUN experiment: results/ \u251c\u2500\u2500 alignment_stats/ \u251c\u2500\u2500 bam/ \u251c\u2500\u2500 bedgraph/ \u251c\u2500\u2500 bigwig/ \u251c\u2500\u2500 fragments/ \u251c\u2500\u2500 peaks/ \u2502 \u251c\u2500\u2500 0.05/ \u2502 \u2502 \u251c\u2500\u2500 contrasts/ \u2502 \u2502 \u251c\u2500\u2500 gopeaks/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2502 \u251c\u2500\u2500 macs2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2502 \u2514\u2500\u2500 seacr/ \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2514\u2500\u2500 0.01/ \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 qc/ \u251c\u2500\u2500 fastqc_raw/ \u2514\u2500\u2500 fqscreen_raw/ \ud83e\udded Tip: The structure is intentionally hierarchical, enabling automated report generation and simplifying downstream integration with visualization tools and statistical frameworks.","title":"Expected Output"},{"location":"user-guide/output/#expected-outputs","text":"Upon successful completion, CARLISLE generates a comprehensive directory structure under WORKDIR/results . Each subdirectory contains outputs corresponding to specific stages of the workflow \u2014 from raw alignment statistics to annotated peak results and quality control summaries.","title":"Expected Outputs"},{"location":"user-guide/output/#directory-overview","text":"alignment_stats/ \u2013 Contains detailed alignment reports for each sample, including mapping efficiency, read depth, and spike-in alignment metrics. bam/ \u2013 Stores sorted and indexed BAM files for all samples. This directory also includes per-sample and spike-in alignment statistics useful for downstream normalization and QC. bedgraph/ \u2013 Includes BEDGRAPH coverage tracks summarizing read density across the genome. These files serve as intermediates for visualization and peak-calling validation. bigwig/ \u2013 Contains BigWig files generated from normalized coverage data, suitable for visualization in genome browsers such as UCSC Genome Browser or IGV . fragments/ \u2013 Stores fragment length distributions and deduplicated fragment data (particularly important for CUT&RUN and CUT&Tag experiments). Useful for assessing fragment size enrichment and MNase digestion efficiency. peaks/ \u2013 The core results directory containing called peaks, differential comparisons, and annotations. Subdirectories are organized by quality thresholds (e.g., 0.05 , 0.01 ), representing the significance cutoffs applied during peak calling. Each quality threshold directory includes: contrasts/ \u2013 Contains results of differential binding analyses defined in the contrast manifest. <peak_caller>/ \u2013 Subdirectories for each peak caller (e.g., macs2 , seacr , gopeaks ). Each includes raw peak calls and annotated results. annotation/ \u2013 Contains enriched feature and pathway analyses: go_enrichment/ \u2013 Results from ChIP-Enrich gene set enrichment, generated when run_go_enrichment: true is enabled. homer/ \u2013 Output from HOMER motif discovery and annotation. rose/ \u2013 Output from ROSE super-enhancer analysis, generated when run_rose: true is specified. qc/ \u2013 Centralized quality control directory containing comprehensive MultiQC summaries, FastQC metrics, and spike-in normalization reports (when applicable).","title":"Directory Overview"},{"location":"user-guide/output/#example-directory-layout","text":"Below is an example of the CARLISLE output structure for a typical CUT&RUN experiment: results/ \u251c\u2500\u2500 alignment_stats/ \u251c\u2500\u2500 bam/ \u251c\u2500\u2500 bedgraph/ \u251c\u2500\u2500 bigwig/ \u251c\u2500\u2500 fragments/ \u251c\u2500\u2500 peaks/ \u2502 \u251c\u2500\u2500 0.05/ \u2502 \u2502 \u251c\u2500\u2500 contrasts/ \u2502 \u2502 \u251c\u2500\u2500 gopeaks/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2502 \u251c\u2500\u2500 macs2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2502 \u2514\u2500\u2500 seacr/ \u2502 \u2502 \u251c\u2500\u2500 annotation/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 go_enrichment/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 homer/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 rose/ \u2502 \u2514\u2500\u2500 0.01/ \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 qc/ \u251c\u2500\u2500 fastqc_raw/ \u2514\u2500\u2500 fqscreen_raw/ \ud83e\udded Tip: The structure is intentionally hierarchical, enabling automated report generation and simplifying downstream integration with visualization tools and statistical frameworks.","title":"Example Directory Layout"},{"location":"user-guide/preparing-files/","text":"Preparing Files \u00b6 The CARLISLE pipeline is configured and controlled through a set of editable configuration and manifest files. Upon initialization, default templates for these files are automatically generated under the /WORKDIR/config and /WORKDIR/manifest directories. \u2699\ufe0f Technical Note: CARLISLE follows a Snakemake-driven workflow architecture where all configuration parameters are read dynamically at runtime. Users are encouraged to version-control configuration files (e.g., via Git) to ensure reproducibility across runs. \ud83d\ude80 Future Development: While dependencies are currently module-loaded on the Biowulf HPC environment , future releases will adopt containerization using Singularity/Apptainer and Docker . This shift will provide complete environment encapsulation, allowing consistent execution across HPC and cloud environments. Configuration Files \u00b6 CARLISLE\u2019s configuration system is modular and designed for both flexibility and transparency. The main configuration files include: config/config.yaml \u2013 global pipeline settings and user parameters. resources/cluster.yaml \u2013 cluster resource specifications for Biowulf or other SLURM-based systems. resources/tools.yaml \u2013 software versions, tool paths, and binary locations. Cluster Configuration ( cluster.yaml ) \u00b6 The cluster configuration file defines computational resources such as memory, CPU cores, and runtime limits for each Snakemake rule. Parameters can be adjusted globally or per rule. Edits should be made with caution, as inappropriate resource settings may cause job failures or queuing delays. Tools Configuration ( tools.yaml ) \u00b6 This file specifies which versions of each tool are used during execution. When running on Biowulf, tools are automatically loaded from environment modules, ensuring consistency across users. Once CARLISLE transitions to containers, these version pins will map to container image tags instead of module versions, guaranteeing strict reproducibility. Primary Configuration ( config.yaml ) \u00b6 The main configuration file ( config.yaml ) contains parameters grouped into logical sections: Folders and Paths: defines input/output directories and manifest file locations. User Parameters: controls feature-level behavior (e.g., thresholds, normalization methods, peak calling options). References: specifies genome assemblies, index paths, spike-in references, and species annotations. \u26a0\ufe0f Important: Always verify that reference genome paths and spike-in references correspond to accessible Biowulf or shared filesystem locations. User Parameters \u00b6 Spike-in Controls \u00b6 CARLISLE supports spike-in normalization using reference genomes such as E. coli or Drosophila melanogaster . The parameter spikein_genome defines the spike-in species, and spikein_reference provides the corresponding FASTA path. Example for E. coli spike-in: run_contrasts : true norm_method : \"spikein\" spikein_genome : \"ecoli\" spikein_reference : ecoli : fa : \"PIPELINE_HOME/resources/spikein/Ecoli_GCF_000005845.2_ASM584v2_genomic.fna\" Example for Drosophila spike-in: run_contrasts : true norm_method : \"spikein\" spikein_genome : \"drosophila\" spikein_reference : drosophila : fa : \"/fdb/igenomes/Drosophila_melanogaster/UCSC/dm6/Sequence/WholeGenomeFasta/genome.fa\" If spike-ins are unavailable or insufficient, normalization can alternatively be performed based on library size. Recommended workflow: Run CARLISLE with norm_method: spikein for an initial QC assessment. Evaluate spike-in alignment statistics. Add alignment_stats to your configuration. Re-run CARLISLE using library-size normalization. Duplication Status \u00b6 Control deduplication behavior using the dupstatus parameter: dupstatus : \"dedup, no_dedup\" \u2705 Recommendation: Keep this setting unchanged, let CARLISLIE run with dedup and no_dedup options and then choose which peakSets to use later. \ud83e\uddec Note: Linear deduplication is essential for CUT&RUN and CUT&Tag datasets to avoid PCR bias and ensure accurate read quantification. Peak Callers \u00b6 CARLISLE supports three major peak callers, configurable via the peaktype parameter: MACS2 \u2013 supports narrowPeak and broadPeak modes. SEACR \u2013 supports stringent and relaxed thresholds, for both normalized and non-normalized datasets. GoPeaks \u2013 optimized for CUT&RUN and CUT&Tag data; recommended for most applications. \u2705 Recommendation: Use GoPeaks for its superior signal detection in sparse chromatin accessibility datasets. Example configuration: peaktype : \"macs2_narrow, gopeaks_narrow\" MACS2 Control Option \u00b6 Enable control sample usage for MACS2 to improve specificity: macs2_control : \"Y\" Quality Thresholds \u00b6 Set peak-calling quality thresholds using the quality_thresholds parameter: quality_thresholds : \"0.1, 0.05, 0.01\" Refer to tool-specific defaults: MACS2 q-value: 0.05 GoPeaks p-value: 0.05 SEACR FDR threshold: 1.0 Reference Files \u00b6 Additional reference genomes can be integrated by defining: species_name : fa : \"/path/to/species.fa\" blacklist : \"/path/to/blacklistbed/species.bed\" regions : \"chr1 chr2 chr3\" macs2_g : \"hs\" # genome shorthand for MACS2 \ud83e\udded Best Practice: Store reference paths under a centralized /fdb or /data location on Biowulf to ensure accessibility and consistency across users. Preparing Manifests \u00b6 CARLISLE uses two manifests: samplemanifest \u2013 required for all analyses. contrasts \u2013 optional, required only for differential analysis with DESeq2. Sample Manifest (Required) \u00b6 Defines sample-level metadata, including sample names, controls, and FASTQ paths. sampleName replicateNumber isControl controlName controlReplicateNumber path_to_R1 path_to_R2 53_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 /53_H3K4me3_1.R1.fastq.gz /53_H3K4me3_1.R2.fastq.gz 54_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 /54_H3K4me3_1.R1.fastq.gz /54_H3K4me3_1.R2.fastq.gz HN6_IgG_rabbit_negative_control 1 Y /HN6_IgG_rabbit_negative_control_1.R1.fastq.gz /HN6_IgG_rabbit_negative_control_2.R2.fastq.gz Contrast Manifest (Optional) \u00b6 Specifies conditions for differential analysis: condition1 condition2 MOC1_siSmyd3_2m_25_HCHO MOC1_siNC_2m_25_HCHO \ud83d\udcca Requirement: Each condition must have at least two biological replicates to perform DESeq2-based differential analysis.","title":"Preparing Files"},{"location":"user-guide/preparing-files/#preparing-files","text":"The CARLISLE pipeline is configured and controlled through a set of editable configuration and manifest files. Upon initialization, default templates for these files are automatically generated under the /WORKDIR/config and /WORKDIR/manifest directories. \u2699\ufe0f Technical Note: CARLISLE follows a Snakemake-driven workflow architecture where all configuration parameters are read dynamically at runtime. Users are encouraged to version-control configuration files (e.g., via Git) to ensure reproducibility across runs. \ud83d\ude80 Future Development: While dependencies are currently module-loaded on the Biowulf HPC environment , future releases will adopt containerization using Singularity/Apptainer and Docker . This shift will provide complete environment encapsulation, allowing consistent execution across HPC and cloud environments.","title":"Preparing Files"},{"location":"user-guide/preparing-files/#configuration-files","text":"CARLISLE\u2019s configuration system is modular and designed for both flexibility and transparency. The main configuration files include: config/config.yaml \u2013 global pipeline settings and user parameters. resources/cluster.yaml \u2013 cluster resource specifications for Biowulf or other SLURM-based systems. resources/tools.yaml \u2013 software versions, tool paths, and binary locations.","title":"Configuration Files"},{"location":"user-guide/preparing-files/#cluster-configuration-clusteryaml","text":"The cluster configuration file defines computational resources such as memory, CPU cores, and runtime limits for each Snakemake rule. Parameters can be adjusted globally or per rule. Edits should be made with caution, as inappropriate resource settings may cause job failures or queuing delays.","title":"Cluster Configuration (cluster.yaml)"},{"location":"user-guide/preparing-files/#tools-configuration-toolsyaml","text":"This file specifies which versions of each tool are used during execution. When running on Biowulf, tools are automatically loaded from environment modules, ensuring consistency across users. Once CARLISLE transitions to containers, these version pins will map to container image tags instead of module versions, guaranteeing strict reproducibility.","title":"Tools Configuration (tools.yaml)"},{"location":"user-guide/preparing-files/#primary-configuration-configyaml","text":"The main configuration file ( config.yaml ) contains parameters grouped into logical sections: Folders and Paths: defines input/output directories and manifest file locations. User Parameters: controls feature-level behavior (e.g., thresholds, normalization methods, peak calling options). References: specifies genome assemblies, index paths, spike-in references, and species annotations. \u26a0\ufe0f Important: Always verify that reference genome paths and spike-in references correspond to accessible Biowulf or shared filesystem locations.","title":"Primary Configuration (config.yaml)"},{"location":"user-guide/preparing-files/#user-parameters","text":"","title":"User Parameters"},{"location":"user-guide/preparing-files/#spike-in-controls","text":"CARLISLE supports spike-in normalization using reference genomes such as E. coli or Drosophila melanogaster . The parameter spikein_genome defines the spike-in species, and spikein_reference provides the corresponding FASTA path. Example for E. coli spike-in: run_contrasts : true norm_method : \"spikein\" spikein_genome : \"ecoli\" spikein_reference : ecoli : fa : \"PIPELINE_HOME/resources/spikein/Ecoli_GCF_000005845.2_ASM584v2_genomic.fna\" Example for Drosophila spike-in: run_contrasts : true norm_method : \"spikein\" spikein_genome : \"drosophila\" spikein_reference : drosophila : fa : \"/fdb/igenomes/Drosophila_melanogaster/UCSC/dm6/Sequence/WholeGenomeFasta/genome.fa\" If spike-ins are unavailable or insufficient, normalization can alternatively be performed based on library size. Recommended workflow: Run CARLISLE with norm_method: spikein for an initial QC assessment. Evaluate spike-in alignment statistics. Add alignment_stats to your configuration. Re-run CARLISLE using library-size normalization.","title":"Spike-in Controls"},{"location":"user-guide/preparing-files/#duplication-status","text":"Control deduplication behavior using the dupstatus parameter: dupstatus : \"dedup, no_dedup\" \u2705 Recommendation: Keep this setting unchanged, let CARLISLIE run with dedup and no_dedup options and then choose which peakSets to use later. \ud83e\uddec Note: Linear deduplication is essential for CUT&RUN and CUT&Tag datasets to avoid PCR bias and ensure accurate read quantification.","title":"Duplication Status"},{"location":"user-guide/preparing-files/#peak-callers","text":"CARLISLE supports three major peak callers, configurable via the peaktype parameter: MACS2 \u2013 supports narrowPeak and broadPeak modes. SEACR \u2013 supports stringent and relaxed thresholds, for both normalized and non-normalized datasets. GoPeaks \u2013 optimized for CUT&RUN and CUT&Tag data; recommended for most applications. \u2705 Recommendation: Use GoPeaks for its superior signal detection in sparse chromatin accessibility datasets. Example configuration: peaktype : \"macs2_narrow, gopeaks_narrow\"","title":"Peak Callers"},{"location":"user-guide/preparing-files/#macs2-control-option","text":"Enable control sample usage for MACS2 to improve specificity: macs2_control : \"Y\"","title":"MACS2 Control Option"},{"location":"user-guide/preparing-files/#quality-thresholds","text":"Set peak-calling quality thresholds using the quality_thresholds parameter: quality_thresholds : \"0.1, 0.05, 0.01\" Refer to tool-specific defaults: MACS2 q-value: 0.05 GoPeaks p-value: 0.05 SEACR FDR threshold: 1.0","title":"Quality Thresholds"},{"location":"user-guide/preparing-files/#reference-files","text":"Additional reference genomes can be integrated by defining: species_name : fa : \"/path/to/species.fa\" blacklist : \"/path/to/blacklistbed/species.bed\" regions : \"chr1 chr2 chr3\" macs2_g : \"hs\" # genome shorthand for MACS2 \ud83e\udded Best Practice: Store reference paths under a centralized /fdb or /data location on Biowulf to ensure accessibility and consistency across users.","title":"Reference Files"},{"location":"user-guide/preparing-files/#preparing-manifests","text":"CARLISLE uses two manifests: samplemanifest \u2013 required for all analyses. contrasts \u2013 optional, required only for differential analysis with DESeq2.","title":"Preparing Manifests"},{"location":"user-guide/preparing-files/#sample-manifest-required","text":"Defines sample-level metadata, including sample names, controls, and FASTQ paths. sampleName replicateNumber isControl controlName controlReplicateNumber path_to_R1 path_to_R2 53_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 /53_H3K4me3_1.R1.fastq.gz /53_H3K4me3_1.R2.fastq.gz 54_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 /54_H3K4me3_1.R1.fastq.gz /54_H3K4me3_1.R2.fastq.gz HN6_IgG_rabbit_negative_control 1 Y /HN6_IgG_rabbit_negative_control_1.R1.fastq.gz /HN6_IgG_rabbit_negative_control_2.R2.fastq.gz","title":"Sample Manifest (Required)"},{"location":"user-guide/preparing-files/#contrast-manifest-optional","text":"Specifies conditions for differential analysis: condition1 condition2 MOC1_siSmyd3_2m_25_HCHO MOC1_siNC_2m_25_HCHO \ud83d\udcca Requirement: Each condition must have at least two biological replicates to perform DESeq2-based differential analysis.","title":"Contrast Manifest (Optional)"},{"location":"user-guide/run/","text":"Running the Pipeline \u00b6 Pipeline Overview \u00b6 The CARLISLE pipeline operates as a modular Snakemake workflow, designed to support flexible execution on both local and cluster environments. It offers several run modes that control initialization, execution, and management of analysis sessions. Required Arguments \u00b6 Usage: carlisle -m/--runmode = <RUNMODE> -w/--workdir = <WORKDIR> 1 . RUNMODE [ string ] : init \u2013 Initialize the working directory run \u2013 Submit jobs to the SLURM cluster ( Biowulf ) reset \u2013 Delete and reinitialize the working directory dryrun \u2013 Validate and preview the workflow ( no jobs executed ) unlock \u2013 Unlock the working directory after an aborted run runlocal \u2013 Execute the pipeline interactively on a local node runtest \u2013 Run the included test dataset on the cluster 2 . WORKDIR [ string ] : Absolute or relative path to the desired output directory with write permissions. Optional Arguments \u00b6 Flag Description --help, -h Display the command-line help message. --version, -v Print the current version of CARLISLE. --force, -f Force re-execution of all Snakemake rules (overrides cache). --singcache, -c Define a custom Singularity cache directory . Defaults to /data/${USER}/.singularity , or ${WORKDIR}/.singularity if unavailable. Command Descriptions \u00b6 \ud83e\udde9 Preparation Commands \u00b6 init (required) \u2013 Initializes the working directory by copying configuration, manifest, and Snakefiles into place. This step must be performed before any other pipeline action. Use the -f or --force flag to reinitialize an existing directory. dryrun (optional) \u2013 Performs a non-executing validation of the Snakemake DAG, checking for syntax issues, missing files, or permission problems before a full run. \u2699\ufe0f Processing Commands \u00b6 runlocal \u2013 Executes the workflow on a local interactive node. This mode is suitable for quick testing or smaller datasets but should only be used within a Biowulf interactive session ( sinteractive ). run \u2013 Submits the workflow to the Biowulf HPC cluster via SLURM. CARLISLE manages job scheduling, dependencies, and notifications. Email alerts are automatically sent for job start, errors, and completion. \ud83e\uddf0 Maintenance Commands \u00b6 unlock \u2013 Unlocks the working directory if Snakemake terminates unexpectedly or a previous job is interrupted. runtest \u2013 Executes a small, bundled test dataset to verify installation and configuration integrity. Usage Syntax \u00b6 All commands follow a consistent syntax: carlisle --runmode = <COMMAND> --workdir = /path/to/output/dir For example: carlisle --runmode = init --workdir = /data/ $USER /project Typical Workflow Example \u00b6 A standard execution sequence on the Biowulf cluster would include the following steps: # Step 1: Initialize working directory carlisle --runmode = init --workdir = /path/to/output/dir # Step 2: Perform a dry run to validate the workflow carlisle --runmode = dryrun --workdir = /path/to/output/dir # Step 3: Submit the full workflow to the cluster carlisle --runmode = run --workdir = /path/to/output/dir \u2705 Recommendation: Always perform a dry run before full execution to verify file paths, environment modules, and configuration correctness.","title":"Running the Pipeline"},{"location":"user-guide/run/#running-the-pipeline","text":"","title":"Running the Pipeline"},{"location":"user-guide/run/#pipeline-overview","text":"The CARLISLE pipeline operates as a modular Snakemake workflow, designed to support flexible execution on both local and cluster environments. It offers several run modes that control initialization, execution, and management of analysis sessions.","title":"Pipeline Overview"},{"location":"user-guide/run/#required-arguments","text":"Usage: carlisle -m/--runmode = <RUNMODE> -w/--workdir = <WORKDIR> 1 . RUNMODE [ string ] : init \u2013 Initialize the working directory run \u2013 Submit jobs to the SLURM cluster ( Biowulf ) reset \u2013 Delete and reinitialize the working directory dryrun \u2013 Validate and preview the workflow ( no jobs executed ) unlock \u2013 Unlock the working directory after an aborted run runlocal \u2013 Execute the pipeline interactively on a local node runtest \u2013 Run the included test dataset on the cluster 2 . WORKDIR [ string ] : Absolute or relative path to the desired output directory with write permissions.","title":"Required Arguments"},{"location":"user-guide/run/#optional-arguments","text":"Flag Description --help, -h Display the command-line help message. --version, -v Print the current version of CARLISLE. --force, -f Force re-execution of all Snakemake rules (overrides cache). --singcache, -c Define a custom Singularity cache directory . Defaults to /data/${USER}/.singularity , or ${WORKDIR}/.singularity if unavailable.","title":"Optional Arguments"},{"location":"user-guide/run/#command-descriptions","text":"","title":"Command Descriptions"},{"location":"user-guide/run/#preparation-commands","text":"init (required) \u2013 Initializes the working directory by copying configuration, manifest, and Snakefiles into place. This step must be performed before any other pipeline action. Use the -f or --force flag to reinitialize an existing directory. dryrun (optional) \u2013 Performs a non-executing validation of the Snakemake DAG, checking for syntax issues, missing files, or permission problems before a full run.","title":"\ud83e\udde9 Preparation Commands"},{"location":"user-guide/run/#processing-commands","text":"runlocal \u2013 Executes the workflow on a local interactive node. This mode is suitable for quick testing or smaller datasets but should only be used within a Biowulf interactive session ( sinteractive ). run \u2013 Submits the workflow to the Biowulf HPC cluster via SLURM. CARLISLE manages job scheduling, dependencies, and notifications. Email alerts are automatically sent for job start, errors, and completion.","title":"\u2699\ufe0f Processing Commands"},{"location":"user-guide/run/#maintenance-commands","text":"unlock \u2013 Unlocks the working directory if Snakemake terminates unexpectedly or a previous job is interrupted. runtest \u2013 Executes a small, bundled test dataset to verify installation and configuration integrity.","title":"\ud83e\uddf0 Maintenance Commands"},{"location":"user-guide/run/#usage-syntax","text":"All commands follow a consistent syntax: carlisle --runmode = <COMMAND> --workdir = /path/to/output/dir For example: carlisle --runmode = init --workdir = /data/ $USER /project","title":"Usage Syntax"},{"location":"user-guide/run/#typical-workflow-example","text":"A standard execution sequence on the Biowulf cluster would include the following steps: # Step 1: Initialize working directory carlisle --runmode = init --workdir = /path/to/output/dir # Step 2: Perform a dry run to validate the workflow carlisle --runmode = dryrun --workdir = /path/to/output/dir # Step 3: Submit the full workflow to the cluster carlisle --runmode = run --workdir = /path/to/output/dir \u2705 Recommendation: Always perform a dry run before full execution to verify file paths, environment modules, and configuration correctness.","title":"Typical Workflow Example"},{"location":"user-guide/test-info/","text":"Pipeline Tutorial \u00b6 Welcome to the CARLISLE Pipeline Tutorial! This guide walks you through running the CARLISLE pipeline using the provided test dataset on the NIH Biowulf HPC environment. Getting Started \u00b6 Before beginning, review the Getting Started Guide for installation, environment setup, and dependency loading instructions. Step 1. Set Your Working Directory \u00b6 Navigate to your project directory on Biowulf: cd /path/to/your/project/directory Step 2. Initialize the Pipeline \u00b6 Load the CARLISLE module and initialize your working directory: module load ccbrpipeliner carlisle --runmode = init --workdir = /path/to/output/dir This command copies the required configuration, manifest, and Snakefiles into your chosen output directory ( WORKDIR ). Initialization must be done before any other CARLISLE operation. Submitting the Test Data \u00b6 The test dataset provided with CARLISLE enables you to validate the installation and confirm correct execution. The test includes minimal FASTQ files, configurations, and manifests. Step 3. Run the Test Command \u00b6 Execute the built-in test run to validate pipeline functionality: carlisle --runmode = runtest --workdir = /path/to/output/dir This command prepares the test data, performs a dry-run to validate workflow dependencies, and then submits the pipeline to the Biowulf SLURM cluster. Expected Output \u00b6 During a successful test run, you should see a job summary similar to the one below, detailing the number of tasks executed per Snakemake rule: Job stats: job count min threads max threads ----------------------------- ------- ------------- ------------- DESeq 24 1 1 align 9 56 56 alignstats 9 2 2 all 1 1 1 bam2bg 9 4 4 create_contrast_data_files 24 1 1 create_contrast_peakcaller_files 12 1 1 create_reference 1 32 32 create_replicate_sample_table 1 1 1 diffbb 24 1 1 filter 18 2 2 findMotif 96 6 6 gather_alignstats 1 1 1 go_enrichment 12 1 1 gopeaks_broad 16 2 2 gopeaks_narrow 16 2 2 macs2_broad 16 2 2 macs2_narrow 16 2 2 make_counts_matrix 24 1 1 multiqc 2 1 1 qc_fastqc 9 1 1 rose 96 2 2 seacr_relaxed 16 2 2 seacr_stringent 16 2 2 spikein_assessment 1 1 1 trim 9 56 56 total 478 1 56 \ud83d\udca1 Tip: This job summary confirms successful rule execution, resource allocation, and workflow orchestratio","title":"Running Test Data"},{"location":"user-guide/test-info/#pipeline-tutorial","text":"Welcome to the CARLISLE Pipeline Tutorial! This guide walks you through running the CARLISLE pipeline using the provided test dataset on the NIH Biowulf HPC environment.","title":"Pipeline Tutorial"},{"location":"user-guide/test-info/#getting-started","text":"Before beginning, review the Getting Started Guide for installation, environment setup, and dependency loading instructions.","title":"Getting Started"},{"location":"user-guide/test-info/#step-1-set-your-working-directory","text":"Navigate to your project directory on Biowulf: cd /path/to/your/project/directory","title":"Step 1. Set Your Working Directory"},{"location":"user-guide/test-info/#step-2-initialize-the-pipeline","text":"Load the CARLISLE module and initialize your working directory: module load ccbrpipeliner carlisle --runmode = init --workdir = /path/to/output/dir This command copies the required configuration, manifest, and Snakefiles into your chosen output directory ( WORKDIR ). Initialization must be done before any other CARLISLE operation.","title":"Step 2. Initialize the Pipeline"},{"location":"user-guide/test-info/#submitting-the-test-data","text":"The test dataset provided with CARLISLE enables you to validate the installation and confirm correct execution. The test includes minimal FASTQ files, configurations, and manifests.","title":"Submitting the Test Data"},{"location":"user-guide/test-info/#step-3-run-the-test-command","text":"Execute the built-in test run to validate pipeline functionality: carlisle --runmode = runtest --workdir = /path/to/output/dir This command prepares the test data, performs a dry-run to validate workflow dependencies, and then submits the pipeline to the Biowulf SLURM cluster.","title":"Step 3. Run the Test Command"},{"location":"user-guide/test-info/#expected-output","text":"During a successful test run, you should see a job summary similar to the one below, detailing the number of tasks executed per Snakemake rule: Job stats: job count min threads max threads ----------------------------- ------- ------------- ------------- DESeq 24 1 1 align 9 56 56 alignstats 9 2 2 all 1 1 1 bam2bg 9 4 4 create_contrast_data_files 24 1 1 create_contrast_peakcaller_files 12 1 1 create_reference 1 32 32 create_replicate_sample_table 1 1 1 diffbb 24 1 1 filter 18 2 2 findMotif 96 6 6 gather_alignstats 1 1 1 go_enrichment 12 1 1 gopeaks_broad 16 2 2 gopeaks_narrow 16 2 2 macs2_broad 16 2 2 macs2_narrow 16 2 2 make_counts_matrix 24 1 1 multiqc 2 1 1 qc_fastqc 9 1 1 rose 96 2 2 seacr_relaxed 16 2 2 seacr_stringent 16 2 2 spikein_assessment 1 1 1 trim 9 56 56 total 478 1 56 \ud83d\udca1 Tip: This job summary confirms successful rule execution, resource allocation, and workflow orchestratio","title":"Expected Output"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting \u00b6 Recommended steps to troubleshoot the pipeline. Email \u00b6 Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=CARLISLE Failed, Run time [time], FAILED, ExitCode 1 Review the log files \u00b6 Review the logs in two ways: Review the master slurm file: This file will be found in the /path/to/results/dir/ and titled slurm-[jobid].out . Reviewing this file will tell you what rule errored, and for any local SLURM jobs, provide error details Review the individual rule log files: After reviewing the master slurm-file, review the specific rules that failed within the /path/to/results/dir/logs/ . Each rule will include a .err and .out file, with the following formatting: {rulename}.{masterjobID}.{individualruleID}.{wildcards from the rule}.{out or err} Restart the run \u00b6 After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster. # unlock dir carlisle --runmode=unlock --workdir=/path/to/output/dir # perform dry-run carlisle --runmode=dryrun --workdir=/path/to/output/dir # submit to cluster carlisle --runmode=run --workdir=/path/to/output/dir Contact information \u00b6 If after troubleshooting, the error cannot be resolved, or if a bug is found, please create an issue or send and email to CCBR_Pipeliner@mail.nih.gov","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#troubleshooting","text":"Recommended steps to troubleshoot the pipeline.","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#email","text":"Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=CARLISLE Failed, Run time [time], FAILED, ExitCode 1","title":"Email"},{"location":"user-guide/troubleshooting/#review-the-log-files","text":"Review the logs in two ways: Review the master slurm file: This file will be found in the /path/to/results/dir/ and titled slurm-[jobid].out . Reviewing this file will tell you what rule errored, and for any local SLURM jobs, provide error details Review the individual rule log files: After reviewing the master slurm-file, review the specific rules that failed within the /path/to/results/dir/logs/ . Each rule will include a .err and .out file, with the following formatting: {rulename}.{masterjobID}.{individualruleID}.{wildcards from the rule}.{out or err}","title":"Review the log files"},{"location":"user-guide/troubleshooting/#restart-the-run","text":"After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster. # unlock dir carlisle --runmode=unlock --workdir=/path/to/output/dir # perform dry-run carlisle --runmode=dryrun --workdir=/path/to/output/dir # submit to cluster carlisle --runmode=run --workdir=/path/to/output/dir","title":"Restart the run"},{"location":"user-guide/troubleshooting/#contact-information","text":"If after troubleshooting, the error cannot be resolved, or if a bug is found, please create an issue or send and email to CCBR_Pipeliner@mail.nih.gov","title":"Contact information"}]}